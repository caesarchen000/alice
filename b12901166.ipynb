{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TFwaJir_Olj"
      },
      "source": [
        "# ML2025 Homework 1 - Retrieval Augmented Generation with Agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tQHdH2k_Olk"
      },
      "source": [
        "## Environment Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_ZkNxqGGhdl"
      },
      "source": [
        "First, we will mount your own Google Drive and change the working directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWQh-lq8GuwZ",
        "outputId": "73c802fc-50d7-4b0e-fe61-d1747c30cb84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_5Tf1rMHBQ-",
        "outputId": "a3573f4a-81d7-43dc-e7f5-5c99ce4fafdb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: '[change to the directory you prefer]'\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "# Change the working directory to somewhere in your Google Drive.\n",
        "# You could check the path by right clicking on the folder.\n",
        "%cd [change to the directory you prefer]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGx000oZ_Oll"
      },
      "source": [
        "In this section, we install the necessary python packages and download model weights of the quantized version of LLaMA 3.1 8B. Also, download the dataset. Note that the model weight is around 8GB. If you are using your Google Drive as the working directory, make sure you have enough space for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JywoPOO_Oll",
        "outputId": "a1342960-9ce0-40fc-8263-bde8ccfc83fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cu122\n",
            "Collecting llama-cpp-python==0.3.4\n",
            "  Downloading https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.4-cu122/llama_cpp_python-0.3.4-cp311-cp311-linux_x86_64.whl (445.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m445.2/445.2 MB\u001b[0m \u001b[31m219.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.3.4) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.3.4) (1.26.4)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python==0.3.4)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.3.4) (3.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python==0.3.4) (3.0.2)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.4\n",
            "Collecting googlesearch-python\n",
            "  Downloading googlesearch_python-1.3.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting bs4\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Collecting requests-html\n",
            "  Downloading requests_html-0.10.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting lxml_html_clean\n",
            "  Downloading lxml_html_clean-0.4.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: beautifulsoup4>=4.9 in /usr/local/lib/python3.11/dist-packages (from googlesearch-python) (4.13.3)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from googlesearch-python) (2.32.3)\n",
            "Collecting pyquery (from requests-html)\n",
            "  Downloading pyquery-2.0.1-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting fake-useragent (from requests-html)\n",
            "  Downloading fake_useragent-2.0.3-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting parse (from requests-html)\n",
            "  Downloading parse-1.20.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting w3lib (from requests-html)\n",
            "  Downloading w3lib-2.3.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting pyppeteer>=0.0.14 (from requests-html)\n",
            "  Downloading pyppeteer-2.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from lxml_html_clean) (5.3.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.9->googlesearch-python) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.9->googlesearch-python) (4.12.2)\n",
            "Collecting appdirs<2.0.0,>=1.4.3 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: certifi>=2023 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html) (2025.1.31)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html) (8.6.1)\n",
            "Collecting pyee<12.0.0,>=11.0.0 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading pyee-11.1.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html) (4.67.1)\n",
            "Collecting urllib3<2.0.0,>=1.25.8 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets<11.0,>=10.0 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading websockets-10.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->googlesearch-python) (3.10)\n",
            "Collecting cssselect>=1.2.0 (from pyquery->requests-html)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html) (3.21.0)\n",
            "Downloading googlesearch_python-1.3.0-py3-none-any.whl (5.6 kB)\n",
            "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Downloading requests_html-0.10.0-py3-none-any.whl (13 kB)\n",
            "Downloading lxml_html_clean-0.4.1-py3-none-any.whl (14 kB)\n",
            "Downloading pyppeteer-2.0.0-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.9/82.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fake_useragent-2.0.3-py3-none-any.whl (201 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.1/201.1 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading parse-1.20.2-py2.py3-none-any.whl (20 kB)\n",
            "Downloading pyquery-2.0.1-py3-none-any.whl (22 kB)\n",
            "Downloading w3lib-2.3.1-py3-none-any.whl (21 kB)\n",
            "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading pyee-11.1.1-py3-none-any.whl (15 kB)\n",
            "Downloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-10.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: parse, appdirs, websockets, w3lib, urllib3, pyee, lxml_html_clean, fake-useragent, cssselect, pyquery, pyppeteer, bs4, requests-html, googlesearch-python\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 14.2\n",
            "    Uninstalling websockets-14.2:\n",
            "      Successfully uninstalled websockets-14.2\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.3.0\n",
            "    Uninstalling urllib3-2.3.0:\n",
            "      Successfully uninstalled urllib3-2.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-genai 1.2.0 requires websockets<15.0dev,>=13.0, but you have websockets 10.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed appdirs-1.4.4 bs4-0.0.2 cssselect-1.2.0 fake-useragent-2.0.3 googlesearch-python-1.3.0 lxml_html_clean-0.4.1 parse-1.20.2 pyee-11.1.1 pyppeteer-2.0.0 pyquery-2.0.1 requests-html-0.10.0 urllib3-1.26.20 w3lib-2.3.1 websockets-10.4\n",
            "Collecting trafilatura\n",
            "  Downloading trafilatura-2.0.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from trafilatura) (2025.1.31)\n",
            "Requirement already satisfied: charset_normalizer>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from trafilatura) (3.4.1)\n",
            "Collecting courlan>=1.3.2 (from trafilatura)\n",
            "  Downloading courlan-1.3.2-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting htmldate>=1.9.2 (from trafilatura)\n",
            "  Downloading htmldate-1.9.3-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting justext>=3.0.1 (from trafilatura)\n",
            "  Downloading justext-3.0.2-py2.py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: lxml>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from trafilatura) (5.3.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.11/dist-packages (from trafilatura) (1.26.20)\n",
            "Requirement already satisfied: babel>=2.16.0 in /usr/local/lib/python3.11/dist-packages (from courlan>=1.3.2->trafilatura) (2.17.0)\n",
            "Collecting tld>=0.13 (from courlan>=1.3.2->trafilatura)\n",
            "  Downloading tld-0.13-py2.py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting dateparser>=1.1.2 (from htmldate>=1.9.2->trafilatura)\n",
            "  Downloading dateparser-1.2.1-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting python-dateutil>=2.9.0.post0 (from htmldate>=1.9.2->trafilatura)\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: pytz>=2024.2 in /usr/local/lib/python3.11/dist-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura) (2025.1)\n",
            "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27,>=2015.06.24 in /usr/local/lib/python3.11/dist-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura) (2024.11.6)\n",
            "Requirement already satisfied: tzlocal>=0.2 in /usr/local/lib/python3.11/dist-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura) (5.3)\n",
            "Requirement already satisfied: lxml_html_clean in /usr/local/lib/python3.11/dist-packages (from lxml[html_clean]>=4.4.2->justext>=3.0.1->trafilatura) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.9.0.post0->htmldate>=1.9.2->trafilatura) (1.17.0)\n",
            "Downloading trafilatura-2.0.0-py3-none-any.whl (132 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading courlan-1.3.2-py3-none-any.whl (33 kB)\n",
            "Downloading htmldate-1.9.3-py3-none-any.whl (31 kB)\n",
            "Downloading justext-3.0.2-py2.py3-none-any.whl (837 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m837.9/837.9 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dateparser-1.2.1-py3-none-any.whl (295 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.7/295.7 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tld-0.13-py2.py3-none-any.whl (263 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m263.8/263.8 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tld, python-dateutil, dateparser, courlan, justext, htmldate, trafilatura\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.8.2\n",
            "    Uninstalling python-dateutil-2.8.2:\n",
            "      Successfully uninstalled python-dateutil-2.8.2\n",
            "Successfully installed courlan-1.3.2 dateparser-1.2.1 htmldate-1.9.3 justext-3.0.2 python-dateutil-2.9.0.post0 tld-0.13 trafilatura-2.0.0\n",
            "--2025-03-04 11:10:29--  https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 18.164.174.17, 18.164.174.55, 18.164.174.118, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.164.174.17|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/d6/e9/d6e9318f285870e2a0e3056e22f9c7ec90cd13e14cfde122129ae66af9ad788f/9da71c45c90a821809821244d4971e5e5dfad7eb091f0b8ff0546392393b6283?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Meta-Llama-3.1-8B-Instruct-Q8_0.gguf%3B+filename%3D%22Meta-Llama-3.1-8B-Instruct-Q8_0.gguf%22%3B&Expires=1741090229&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MTA5MDIyOX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2Q2L2U5L2Q2ZTkzMThmMjg1ODcwZTJhMGUzMDU2ZTIyZjljN2VjOTBjZDEzZTE0Y2ZkZTEyMjEyOWFlNjZhZjlhZDc4OGYvOWRhNzFjNDVjOTBhODIxODA5ODIxMjQ0ZDQ5NzFlNWU1ZGZhZDdlYjA5MWYwYjhmZjA1NDYzOTIzOTNiNjI4Mz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=EMxUepV97c4vehSY-S9UiQI%7E8vREyrqvcb70%7EuvFrwOc%7EnnoOyA2Vmi2APbbNkiZF24Vi951GJ9IYfVw7r9c2RCQb8xBPPLPFGWEjNsace6IZrsbRH3YJneB37zSuHYdzH5GcY1f8fr%7EcvyFH9%7EpqdFJpabsIxlBJmgQcC701JgvvwZMoCVzPfaaY5zW74CX4sZHmQbLGaE7rbt5N6c8jP7vSX%7ExIzFz4O1q%7EK2x8oqcfPFDKsJ3BrGbH2Xmt19Ny-elDozfUJUrBScZJx1oid3Cj0ZlQlXTkbazcRso-1XGX6vMHXjSxmFqgE9UAACN19fr%7EgvSAv65amKwNnWNOw__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2025-03-04 11:10:29--  https://cdn-lfs-us-1.hf.co/repos/d6/e9/d6e9318f285870e2a0e3056e22f9c7ec90cd13e14cfde122129ae66af9ad788f/9da71c45c90a821809821244d4971e5e5dfad7eb091f0b8ff0546392393b6283?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Meta-Llama-3.1-8B-Instruct-Q8_0.gguf%3B+filename%3D%22Meta-Llama-3.1-8B-Instruct-Q8_0.gguf%22%3B&Expires=1741090229&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MTA5MDIyOX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2Q2L2U5L2Q2ZTkzMThmMjg1ODcwZTJhMGUzMDU2ZTIyZjljN2VjOTBjZDEzZTE0Y2ZkZTEyMjEyOWFlNjZhZjlhZDc4OGYvOWRhNzFjNDVjOTBhODIxODA5ODIxMjQ0ZDQ5NzFlNWU1ZGZhZDdlYjA5MWYwYjhmZjA1NDYzOTIzOTNiNjI4Mz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=EMxUepV97c4vehSY-S9UiQI%7E8vREyrqvcb70%7EuvFrwOc%7EnnoOyA2Vmi2APbbNkiZF24Vi951GJ9IYfVw7r9c2RCQb8xBPPLPFGWEjNsace6IZrsbRH3YJneB37zSuHYdzH5GcY1f8fr%7EcvyFH9%7EpqdFJpabsIxlBJmgQcC701JgvvwZMoCVzPfaaY5zW74CX4sZHmQbLGaE7rbt5N6c8jP7vSX%7ExIzFz4O1q%7EK2x8oqcfPFDKsJ3BrGbH2Xmt19Ny-elDozfUJUrBScZJx1oid3Cj0ZlQlXTkbazcRso-1XGX6vMHXjSxmFqgE9UAACN19fr%7EgvSAv65amKwNnWNOw__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 18.65.25.95, 18.65.25.42, 18.65.25.21, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|18.65.25.95|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8540775840 (8.0G) [binary/octet-stream]\n",
            "Saving to: ‘Meta-Llama-3.1-8B-Instruct-Q8_0.gguf’\n",
            "\n",
            "Meta-Llama-3.1-8B-I 100%[===================>]   7.95G   206MB/s    in 41s     \n",
            "\n",
            "2025-03-04 11:11:11 (198 MB/s) - ‘Meta-Llama-3.1-8B-Instruct-Q8_0.gguf’ saved [8540775840/8540775840]\n",
            "\n",
            "--2025-03-04 11:11:11--  https://www.csie.ntu.edu.tw/~ulin/public.txt\n",
            "Resolving www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)... 140.112.30.26\n",
            "Connecting to www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)|140.112.30.26|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4399 (4.3K) [text/plain]\n",
            "Saving to: ‘public.txt’\n",
            "\n",
            "public.txt          100%[===================>]   4.30K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-03-04 11:11:11 (243 MB/s) - ‘public.txt’ saved [4399/4399]\n",
            "\n",
            "--2025-03-04 11:11:12--  https://www.csie.ntu.edu.tw/~ulin/private.txt\n",
            "Resolving www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)... 140.112.30.26\n",
            "Connecting to www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)|140.112.30.26|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15229 (15K) [text/plain]\n",
            "Saving to: ‘private.txt’\n",
            "\n",
            "private.txt         100%[===================>]  14.87K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-03-04 11:11:12 (110 KB/s) - ‘private.txt’ saved [15229/15229]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!python3 -m pip install --no-cache-dir llama-cpp-python==0.3.4 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122\n",
        "!python3 -m pip install googlesearch-python bs4 charset-normalizer requests-html lxml_html_clean\n",
        "!python3 -m pip install trafilatura\n",
        "from pathlib import Path\n",
        "if not Path('./Meta-Llama-3.1-8B-Instruct-Q8_0.gguf').exists():\n",
        "    !wget https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\n",
        "if not Path('./public.txt').exists():\n",
        "    !wget https://www.csie.ntu.edu.tw/~ulin/public.txt\n",
        "if not Path('./private.txt').exists():\n",
        "    !wget https://www.csie.ntu.edu.tw/~ulin/private.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kX6SizAt_Olm",
        "outputId": "21608615-fd30-4f9a-e33e-c052c851c61b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are good to go!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "if not torch.cuda.is_available():\n",
        "    raise Exception('You are not using the GPU runtime. Change it first or you will suffer from the super slow inference speed!')\n",
        "else:\n",
        "    print('You are good to go!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3iyc1qC_Olm"
      },
      "source": [
        "## Prepare the LLM and LLM utility function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T59vxAo2_Olm"
      },
      "source": [
        "By default, we will use the quantized version of LLaMA 3.1 8B. you can get full marks on this homework by using the provided LLM and LLM utility function. You can also try out different LLM models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtepTeT3_Olm"
      },
      "source": [
        "In the following code block, we will load the downloaded LLM model weights onto the GPU first.\n",
        "Then, we implemented the generate_response() function so that you can get the generated response from the LLM model more easily."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVil2Vhe_Olm"
      },
      "source": [
        "You can ignore \"llama_new_context_with_model: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\" warning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScyW45N__Olm",
        "outputId": "5f949564-95ae-40eb-cd66-62324e13015c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_new_context_with_model: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
          ]
        }
      ],
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "# Load the model onto GPU\n",
        "llama3 = Llama(\n",
        "    \"./Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\",\n",
        "    verbose=False,\n",
        "    n_gpu_layers=-1,\n",
        "    n_ctx=16384,    # This argument is how many tokens the model can take. The longer the better, but it will consume more memory. 16384 is a proper value for a GPU with 16GB VRAM.\n",
        ")\n",
        "\n",
        "def generate_response(_model: Llama, _messages: str) -> str:\n",
        "    '''\n",
        "    This function will inference the model with given messages.\n",
        "    '''\n",
        "    _output = _model.create_chat_completion(\n",
        "        _messages,\n",
        "        stop=[\"<|eot_id|>\", \"<|end_of_text|>\"],\n",
        "        max_tokens=512,    # This argument is how many tokens the model can generate, you can change it and observe the differences.\n",
        "        temperature=0,      # This argument is the randomness of the model. 0 means no randomness. You will get the same result with the same input every time. You can try to set it to different values.\n",
        "        repeat_penalty=2.0,\n",
        "    )[\"choices\"][0][\"message\"][\"content\"]\n",
        "    return _output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnHLwq-4_Olm"
      },
      "source": [
        "## Search Tool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYM-2ZsE_Olm"
      },
      "source": [
        "The TA has implemented a search tool for you to search certain keywords using Google Search. You can use this tool to search for the relevant **web pages** for the given question. The search tool can be integrated in the following sections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bEIRmZl7_Oln"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from googlesearch import search as _search\n",
        "from bs4 import BeautifulSoup\n",
        "from charset_normalizer import detect\n",
        "import asyncio\n",
        "from requests_html import AsyncHTMLSession\n",
        "import urllib3\n",
        "urllib3.disable_warnings()\n",
        "\n",
        "async def worker(s:AsyncHTMLSession, url:str):\n",
        "    try:\n",
        "        header_response = await asyncio.wait_for(s.head(url, verify=False), timeout=10)\n",
        "        if 'text/html' not in header_response.headers.get('Content-Type', ''):\n",
        "            return None\n",
        "        r = await asyncio.wait_for(s.get(url, verify=False), timeout=10)\n",
        "        return r.text\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "async def get_htmls(urls):\n",
        "    session = AsyncHTMLSession()\n",
        "    tasks = (worker(session, url) for url in urls)\n",
        "    return await asyncio.gather(*tasks)\n",
        "\n",
        "async def search(keyword: str, n_results: int=3) -> List[str]:\n",
        "    '''\n",
        "    This function will search the keyword and return the text content in the first n_results web pages.\n",
        "\n",
        "    Warning: You may suffer from HTTP 429 errors if you search too many times in a period of time. This is unavoidable and you should take your own risk if you want to try search more results at once.\n",
        "    The rate limit is not explicitly announced by Google, hence there's not much we can do except for changing the IP or wait until Google unban you (we don't know how long the penalty will last either).\n",
        "    '''\n",
        "    keyword = keyword[:100]\n",
        "    # First, search the keyword and get the results. Also, get 2 times more results in case some of them are invalid.\n",
        "    results = list(_search(keyword, n_results * 3, lang=\"zh\", unique=True))\n",
        "    # Then, get the HTML from the results. Also, the helper function will filter out the non-HTML urls.\n",
        "    results = await get_htmls(results)\n",
        "    # Filter out the None values.\n",
        "    results = [x for x in results if x is not None]\n",
        "    # Parse the HTML.\n",
        "    results = [BeautifulSoup(x, 'html.parser') for x in results]\n",
        "    # Get the text from the HTML and remove the spaces. Also, filter out the non-utf-8 encoding.\n",
        "    results = [''.join(x.get_text().split()) for x in results if detect(x.encode()).get('encoding') == 'utf-8']\n",
        "    # Return the first n results.\n",
        "    return results[:n_results]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rC3zQjjj_Oln"
      },
      "source": [
        "## Test the LLM inference pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dmGCARd_Oln",
        "outputId": "fd56dc68-b268-4781-9daa-2436ab8979a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "我無法提供最新的價格資訊，因為市場上商品和服務可能會有變動。然而，我可以告訴你，RTX 5090是一款高端顯示卡，由NVIDIA推出。\n",
            "\n",
            "根據目前可得知的情況，這些是 RTx系列的一部分，但我無法提供最新的價格資訊。如果您想知道最新的市場報導或購買建議，我會推薦查詢線上商店、專業評論家，或直接聯繫NVIDIA官方網站。\n"
          ]
        }
      ],
      "source": [
        "# You can try out different questions here.\n",
        "test_question='RTX 5090多少錢'\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"你是 LLaMA-3.1-8B，是用來回答問題的 AI。使用中文時只會使用繁體中文來回問題。\"},    # System prompt\n",
        "    {\"role\": \"user\", \"content\": test_question}, # User prompt\n",
        "]\n",
        "\n",
        "print(generate_response(llama3, messages))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0-ojJuE_Oln"
      },
      "source": [
        "## Agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGsIPud3_Oln"
      },
      "source": [
        "The TA has implemented the Agent class for you. You can use this class to create agents that can interact with the LLM model. The Agent class has the following attributes and methods:\n",
        "- Attributes:\n",
        "    - role_description: The role of the agent. For example, if you want this agent to be a history expert, you can set the role_description to \"You are a history expert. You will only answer questions based on what really happened in the past. Do not generate any answer if you don't have reliable sources.\".\n",
        "    - task_description: The task of the agent. For example, if you want this agent to answer questions only in yes/no, you can set the task_description to \"Please answer the following question in yes/no. Explanations are not needed.\"\n",
        "    - llm: Just an indicator of the LLM model used by the agent.\n",
        "- Method:\n",
        "    - inference: This method takes a message as input and returns the generated response from the LLM model. The message will first be formatted into proper input for the LLM model. (This is where you can set some global instructions like \"Please speak in a polite manner\" or \"Please provide a detailed explanation\".) The generated response will be returned as the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjG-UwDX_Oln"
      },
      "outputs": [],
      "source": [
        "class LLMAgent():\n",
        "    def __init__(self, role_description: str, task_description: str, llm:str=\"bartowski/Meta-Llama-3.1-8B-Instruct-GGUF\",temperature=0.2,max_tokens=512, verbose=False):\n",
        "        self.role_description = role_description   # Role means who this agent should act like. e.g. the history expert, the manager......\n",
        "        self.task_description = task_description    # Task description instructs what task should this agent solve.\n",
        "        self.temperature=temperature\n",
        "        self.verbose=verbose\n",
        "        self.max_tokens=max_tokens\n",
        "        self.llm = llm  # LLM indicates which LLM backend this agent is using.\n",
        "    def inference(self, message:str) -> str:\n",
        "        if self.llm == 'bartowski/Meta-Llama-3.1-8B-Instruct-GGUF': # If using the default one.\n",
        "            # TODO: Design the system prompt and user prompt here.\n",
        "            # Format the messsages first.\n",
        "            if self.verbose:\n",
        "              print(f\" Agent Role {self.role_description}\")\n",
        "              print(f\" Tasks: {self.task_description}\")\n",
        "              print(f\" User message: {message}\")\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": f\"你的角色：{self.role_description}，請用繁體中文回答\"},  # Hint: you may want the agents to speak Traditional Chinese only.\n",
        "                {\"role\": \"user\", \"content\": f\"你的任務：{self.task_description}\\n 訊息，資料：{message}\"}, # Hint: you may want the agents to clearly distinguish the task descriptions and the user messages. A proper seperation text rather than a simple line break is recommended.\n",
        "            ]\n",
        "            return generate_response(llama3, messages)\n",
        "        else:\n",
        "            # TODO: If you want to use LLMs other than the given one, please implement the inference part on your own.\n",
        "            return \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-ueJrgP_Oln"
      },
      "source": [
        "TODO: Design the role description and task description for each agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DzPzmNnj_Oln"
      },
      "outputs": [],
      "source": [
        "# TODO: Design the role and task description for each agent.\n",
        "\n",
        "# This agent may help you filter out the irrelevant parts in question descriptions.\n",
        "question_extraction_agent = LLMAgent(\n",
        "    role_description=\"你是一個精通問題分析的AI，專門負責提取關鍵問題的部分，並且避免回覆與問題無關的內容\",\n",
        "    task_description=\"\"\"1. 請從以下問題中提取核心問題，刪除不相關資訊\n",
        "                        2. 請不要回答問題答案\n",
        "                        3. 生成回覆的時候請直接回答該問題，不要有廢話\n",
        "                    \"\"\",\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# This agent may help you extract the keywords in a question so that the search tool can find more accurate results.\n",
        "keyword_extraction_agent = LLMAgent(\n",
        "    role_description=\"你是個專門提取關鍵字的AI，負責從問題中找出最關鍵的關鍵字，以便進行有效的網路搜尋\",\n",
        "    task_description=\"\"\"1. 請從以下問題中取出重要關鍵字，去掉助詞及不必要的敘述\n",
        "                        2. 取出重要關鍵字的額外規則:若問題中有類似\"最\"、\"多少\"、\"多久\"、\"多長\"、\"是誰\"、\"在哪裡\"、\"第一\"、\"最後\"、\"誰的\"、\"哪個\"，則這些詞必須被列為關鍵字\n",
        "                        3. 請不要回答問題\n",
        "                        4. 回答的時候請只說關鍵字，中間請以頓號分開\n",
        "                        5. 若題目中有\"依據...\"，則也要將\"依據...\"列為關鍵字\n",
        "                     \"\"\",\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# This agent is the core component that answers the question.\n",
        "qa_agent = LLMAgent(\n",
        "    role_description=\"你是 LLaMA-3.1-8B，是用來回答問題的 AI。使用中文時只會使用繁體中文來回問題。\",\n",
        "    task_description=\"\"\"以下列點為將給你的資訊:\n",
        "              1. 先給你你要回答的題目\n",
        "              2. 網路上搜尋到的資訊\n",
        "              目標:請依1.要回答的題目，再從2.去尋找用來回答的正確答案\n",
        "              回答方式要求:要以精簡的方式回答，且回答的語言為繁體中文，若答案有人名，則僅限人名的部分可用英文回答，不得使用簡體中文與殘體中文\n",
        "              尋找答案方法提示:裡面會有一大堆跟正確答案不相關的資訊，從中過濾出必要資訊。舉例:在整個台灣本島中，有幾個直轄市? 那你去分析這個問題的時候就要先知道整個範圍是在台灣本島，而不是泰國，日本這些國家中的直轄市數量，目標是去尋找直轄市數量。\n",
        "              回答格式要求:請提供明確答案，答案只會有一個正確的，不會有什麼答案是A和B都是這種，不要重複問題，若無法回答請不要說「我不知道」，而是以你原本就知道的知識回答。舉例:題目:台灣首都在哪裡? 你的回答是台北市，而非台北市和新北市(請不要有這種有多於一個答案的回答)\n",
        "              另外你的回答不能出現任何問句，若你想要用問句回答則請用你原本的資料庫的答案回答\n",
        "              你的回答請多解釋一兩句話，但答案的關鍵字一定要有\"\"\",\n",
        "    verbose=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9eoywr7_Oln"
      },
      "source": [
        "## RAG pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vPCZag5fxrMe"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "def fetch_html(url):\n",
        "    \"\"\" Fetches clean text from a webpage using requests & BeautifulSoup. \"\"\"\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Failed to fetch {url}, Status Code: {response.status_code}\")\n",
        "            return None\n",
        "\n",
        "        # Parse HTML\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "        # Remove unnecessary elements\n",
        "        for script in soup([\"script\", \"style\", \"header\", \"footer\", \"nav\", \"aside\"]):\n",
        "            script.extract()\n",
        "\n",
        "        # Extract visible text\n",
        "        text = soup.get_text(separator=\" \")\n",
        "\n",
        "        # Clean up extra spaces\n",
        "        clean_text = \" \".join(text.split())\n",
        "\n",
        "        return clean_text[:10000]  # Truncate to avoid excessive length\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching {url}: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HDOjNYJ_Oln"
      },
      "source": [
        "TODO: Implement the RAG pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRGNa-1i_Oln"
      },
      "source": [
        "Please refer to the homework description slides for hints.\n",
        "\n",
        "Also, there might be more heuristics (e.g. classifying the questions based on their lengths, determining if the question need a search or not, reconfirm the answer before returning it to the user......) that are not shown in the flow charts. You can use your creativity to come up with a better solution!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMaIsKAZ_Olo"
      },
      "source": [
        "- Naive approach (simple baseline)\n",
        "\n",
        "    ![](https://www.csie.ntu.edu.tw/~ulin/naive.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mppO-oOO_Olo"
      },
      "source": [
        "- Naive RAG approach (medium baseline)\n",
        "\n",
        "    ![](https://www.csie.ntu.edu.tw/~ulin/naive_rag.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYxbciLO_Olo"
      },
      "source": [
        "- RAG with agents (strong baseline)\n",
        "\n",
        "    ![](https://www.csie.ntu.edu.tw/~ulin/rag_agent.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ztJkA7R7_Olo"
      },
      "outputs": [],
      "source": [
        "async def pipeline(question: str) -> str:\n",
        "    # TODO: Implement your pipeline.\n",
        "    # Currently, it only feeds the question directly to the LLM.\n",
        "    # You may want to get the final results through multiple inferences.\n",
        "    # Just a quick reminder, make sure your input length is within the limit of the model context window (16384 tokens), you may want to truncate some excessive texts.\n",
        "    # print(\"=== Step 1: Extracting Core Question ===\")\n",
        "    # core_question=question_extraction_agent.inference(question)\n",
        "    # print(f\"Extracted Question: {core_question}\")\n",
        "\n",
        "    # print(\"=== Step 2: Extracting Keywords for Search ===\")\n",
        "    # search_keywords=keyword_extraction_agent.inference(core_question)\n",
        "    # print(f\"Search Keywords:{search_keywords}\")\n",
        "\n",
        "    # print(\"=== Step 3: Retrieving Relevant Information ===\")\n",
        "    # search_results=list(_search(search_keywords,num_results=3))\n",
        "    # print(f\"Search Results: {search_results}\")\n",
        "    # if not search_results:\n",
        "    #   print(\"No relevant search results found. The model will attempt to answer based n its internal knowledge.\")\n",
        "    #   retrieved_text=\"無搜尋結果，請根據內部知識回答\"\n",
        "    # else:\n",
        "    #   retrieved_texts=[fetch_html(url) for url in search_results]\n",
        "    #   retrieved_texts=[text for text in retrieved_texts if text]\n",
        "    #   retrieved_text=\"\\n\\n\".join(retrieved_texts)[:16000]\n",
        "    #   print(f\"Retrieved Text: {retrieved_text}\")\n",
        "    core_question=question_extraction_agent.inference(question)\n",
        "    print(f\"core question:{core_question}\")\n",
        "    search_keywords=keyword_extraction_agent.inference(core_question)\n",
        "    print(f\"search keywords:{search_keywords}\")\n",
        "    support_keywords=keyword_extraction_agent.inference(question)\n",
        "    # print(f\"support keywords{support_keywords}\")\n",
        "    search_results=await search(search_keywords)\n",
        "    # support_results=support_agent.inference(support_keywords)\n",
        "    MAX_CONTEXT_SIZE = 14000  # Leave space for question and system prompt\n",
        "    print('\\n')\n",
        "    # Ensure the text fits within the model’s limit\n",
        "    retrieved_text = \"\\n\\n\".join(search_results)  # Join all search results into one text block\n",
        "    retrieved_text = retrieved_text[:MAX_CONTEXT_SIZE] if len(retrieved_text) > MAX_CONTEXT_SIZE else retrieved_text\n",
        "    # print(\"=== Step 4 : Answering the Question ===\")\n",
        "    qa_prompt= f\"\"\"\n",
        "    1. 問題：{core_question}\n",
        "    2. 請根據以下檢索結果回答問題：\n",
        "    ===============================\n",
        "    {retrieved_text}\n",
        "    ===============================\n",
        "    \"\"\"\n",
        "    answer=qa_agent.inference(qa_prompt)\n",
        "    # print(f\"Answer: {answer}\")\n",
        "    return answer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_kI_9EGB0S9"
      },
      "source": [
        "## Answer the questions using your pipeline!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PN17sSZ8DUg7"
      },
      "source": [
        "Since Colab has usage limit, you might encounter the disconnections. The following code will save your answer for each question. If you have mounted your Google Drive as instructed, you can just rerun the whole notebook to continue your process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plUDRTi_B39S",
        "outputId": "c73df640-24ce-4d1d-eb31-6ac2f009f853"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "core question:核心問題：「虎山雄風飛揚」是哪間學校的校歌？\n",
            "search keywords:虎山雄風飛揚; 校歌\n",
            "\n",
            "\n",
            "1 虎山雄風飛揚是光華國小的校歌。\n",
            "core question:核心問題：2025年初，NCC規定民眾透過境外郵購自用產品回台加收審查費多少錢？\n",
            "search keywords:2025年初、NCC規定民眾透過境外郵購自用產品回台加收審查費多少錢？\n",
            "\n",
            "最關鍱字： \n",
            "# N CC # 境 外 郡 購 自 用 產 品 回 台 加 收 審 查 費\n",
            "\n",
            "\n",
            "2 NCC規定民眾透過境外郵購自用產品回台加收審查費750元，僅適用於通過寄送的案件，而非攜帶入國。\n",
            "core question:核心問題：第一代 iPhone 是由哪位蘋果 CEO 發表？\n",
            "search keywords:第一代 iPhone · 蘋果 CEO\n",
            "\n",
            "\n",
            "3 史蒂夫·乔布斯是第一代 iPhone 的发表者。\n",
            "core question:核心問題：托福網路測驗 TOEFL iBT 要達到多少分才能申請進階英文免修？\n",
            "search keywords:托福網路測驗; TOEFL iBT ; 免修\n",
            "\n",
            "\n",
            "4 托福網路測驗 TOEFL iBT 要達到多少分才能申請進階英文免修？  根據國立臺南大學通識教育中心的資訊，TOFELi BT 72 分以上可獲得大一學生及轉入生的「英語」課程全額退費。\n",
            "core question:核心問題：Rugby Union 中觸地 try 可得幾分？\n",
            "search keywords:觸地 try、Rugby Union\n",
            "\n",
            "\n",
            "5 觸地 try 可得 5 分。\n",
            "core question:核心問題：卑南族的祖先發源地位於現今哪個行政區劃？\n",
            "search keywords:卑南族 # 祖先發源地 依據資料\n",
            "\n",
            "\n",
            "6 卑南族的祖先發源地位於現今台東縣太麻里鄉美和村附近。\n",
            "core question:核心問題：熊仔的碩班指導教授為誰？\n",
            "search keywords:熊仔 · 碩班指導教授\n",
            "\n",
            "\n",
            "7 熊仔的碩班指導教授為李琳山。\n",
            "core question:核心問題：誰發現了電磁感應定律？\n",
            "search keywords:誰 · 發現了電磁感應定律\n",
            "\n",
            "\n",
            "8 迈克尔·法拉第\n",
            "core question:核心問題：距離國立臺灣史前文化博物館最近的臺鐵車站為？\n",
            "search keywords:距離  • 國立臺灣史前文化博物館最近的臺鐵車站為？ \n",
            "\n",
            "最關鍵字： \n",
            "國民航空總局\n",
            "台北火车总站在1921年建成，原名为“国都铁路”\n",
            "\n",
            "\n",
            "9 距離國立臺灣史前文化博物館最近的臺鐵車站為康樂站在台東市，徒步5至10分鐘即可抵達。\n",
            "core question:核心問題：20+30=?\n",
            "search keywords:20、30\n",
            "\n",
            "\n",
            "10 20+30=50\n",
            "core question:核心問題：達拉斯獨行俠隊的Luka Doncic被交易至哪一聯盟中的球队？\n",
            "search keywords:達拉斯·獨行俠隊; Luka Doncic ; 交易至哪一聯盟中的球队\n",
            "\n",
            "\n",
            "11 卢卡·东契奇被交易至NBA西部联盟的洛杉矶湖人队。\n",
            "core question:核心問題：2024年美國總統大選的勝选人是誰？\n",
            "search keywords:2024年 美國 總統 大選 勝选人 是誰\n",
            "\n",
            "\n",
            "12 2024年美國總統大選的勝选人是唐納·川普。\n",
            "core question:核心問題：Meta 的 Llama-3.2 系列模型中，參數量最小的哪個？\n",
            "search keywords:最、參數量、小\n",
            "\n",
            "\n",
            "13 根据检索结果，Meta 的 LLaMA-3.2 系列模型中参数量最小的为 MiniMind-small-T 模型，其大小仅有 26M。\n",
            "core question:核心問題：依據國立臺灣大學學則，停修有多嚴格的限制？\n",
            "search keywords:停修；國立臺灣大學學則; 限制\n",
            "\n",
            "\n",
            "14 依據國立臺灣大學學則，停修課程的限制是嚴格禁止。\n",
            "core question:核心問題：DeepSeek公司的母 公司是誰？\n",
            "search keywords:DeepSeek公司 · 母 公司\n",
            "\n",
            "\n",
            "15 DeepSeek的母公司是幻方（High-Flyer）。\n",
            "core question:核心問題：2024年NBA的總冠軍隊伍是哪一队？\n",
            "search keywords:2024年; NBA ; 總冠軍\n",
            "\n",
            "\n",
            "16 2024年NBA總冠軍隊伍是波士頓凱爾特人。\n",
            "core question:核心問題：一個碳氫化合物分子中有兩個或以上的原子的鍵數超過三，該類型為什麼？\n",
            "search keywords:碳氫化合物 # 鍵數超過三\n",
            "\n",
            "\n",
            "17 一個碳氫化合物分子中有兩個或以上的原子的鍵數超過三，該類型為烷基鏈。\n",
            "core question:提取核心問題：被譽為「計算機科學之父」，提出圖靈machine概念的人是誰？\n",
            "search keywords:圖靈，計算機科學之父\n",
            "\n",
            "\n",
            "18 艾伦·图灵被誉为「计算机科學之父」，提出圖靈機概念的人是他。\n",
            "core question:核心問題：臺灣玄天上帝信仰的進香中心位於哪個行政區劃內？\n",
            "search keywords:關鍵字：臺灣、玄天上帝信仰的進香中心在哪個行政區劃內\n",
            "\n",
            "\n",
            "19 受天宮玄 天上帝香期的進 香中心位於南投縣名間鄉。\n",
            "core question:核心問題：Windows 作業系統是哪間科技公司的產品？\n",
            "search keywords:Windows 作業系統 # Microsoft\n",
            "\n",
            "\n",
            "20 Windows 作業系統是微軟公司的產品。\n",
            "core question:核心問題：官將首起源自哪間廟宇？\n",
            "search keywords:官將 # 首起源自哪間廟宇\n",
            "\n",
            "\n",
            "21 官將首起源自新北市的哪間廟宇？  答案： 新莊地藏庵\n",
            "core question:提取核心問題：《咒》的邪神名為？\n",
            "search keywords:咒、邪神\n",
            "\n",
            "\n",
            "22 《咒》的邪神名為大黑佛母。\n",
            "core question:核心問題：短暫交會的旅程就此分岔是哪個歌唱團體(song group) 的曲詞？\n",
            "search keywords:短暫交會 依據... 是誰 的 曲詞\n",
            "\n",
            "\n",
            "23 短暫交會的旅程就此分岔是哪個歌唱團體(song group) 的曲詞？  根據檢索結果，我們發現這首名為《Short Trip》的音樂作品，實際上並不是一個已知的大型樂隊或組合，而是一種獨立創作。然而，在網路上的資訊中，有提到一位日本歌手叫做 Haruki Murakami，他曾經在他的小說「海邊的卡夫凱」(Kafka on the Shore) 中，描述了一個名為《Short Trip》的短暫旅程。  因此，我們可以得出結論，這首曲詞可能是由Harukimurkamii創作，而不是一個已知的大型樂隊或組合。\n",
            "core question:核心問題：2025 卑南族聯合年聚在哪個部落舉辦？\n",
            "search keywords:2025年、卑南族聯合會議\n",
            "\n",
            "\n",
            "24 根據檢索結果，我們可以得知2025卑南族聯合年祭將於2月1日（大年的初四）在利嘉部落舉辦，主題為「narunirruninatawlriyulr臀鈴的呼喚」。\n",
            "core question:核心問題：最新的輝達顯卡是出到哪一系列？\n",
            "search keywords:最新的輝達顯卡；系列\n",
            "\n",
            "\n",
            "25 GeForce RTX50系列\n",
            "core question:提取核心問題：大S在哪個國家旅遊時去世？\n",
            "search keywords:大S · 旅遊時死亡\n",
            "\n",
            "\n",
            "26 大S在日本旅游时去世，享年48岁。\n",
            "core question:核心問題：誰發現了萬有引力？\n",
            "search keywords:誰 · 萬有引力\n",
            "\n",
            "\n",
            "27 艾萨克·牛顿爵士\n",
            "core question:核心問題：台鵠開示計畫「TAIHUCAIS」的英文全名為何？\n",
            "search keywords:TAIHUCAIS · 台鵠開示計畫\n",
            "\n",
            "\n",
            "28 台鵠開示計畫「TAIHUCAIS」的英文全名為：Taiwan Humanities Conversational AI Knowledge Discovery System\n",
            "core question:核心問題：「I'll be back」是出自哪部電影的經典台詞？\n",
            "search keywords:最　經典台詞　\n",
            "出自哪部電影　  是誰的　　  台語\n",
            "\n",
            "\n",
            "29 《乱世佳人》中的“坦白地说，我不在乎”。\n",
            "core question:核心問題：水的化學式為？\n",
            "search keywords:水的化學式、核心問題\n",
            "\n",
            "\n",
            "30 水的化學式為H2O。\n",
            "core question:核心問題：李宏毅在台灣大學開設的《機器學習》 2023 年春季班中，第15個作業名稱是什麼？\n",
            "search keywords:李宏毅 # 台灣大學 依據... 《機器學習》 春季班 作業名稱 第15個\n",
            "\n",
            "\n",
            "31 根据检索结果，第 15 个作业的名称是 \"HW3：CNN\"。\n",
            "core question:該獨立學院為何？\n",
            "search keywords:獨立學院\n",
            "\n",
            "\n",
            "32 独立学院是指实施本科以上学历教育的普通高等学校与国家机构以外社会组织或个人合作，利用非财政性经费举办的一种民辦高校。\n",
            "core question:核心問題：BitTorrent 協議如何確保新節點能夠從其他種子隨機獲得部分資料？\n",
            "search keywords:BitTorrent 協議、確保、新節點獲得部分資料\n",
            "\n",
            "\n",
            "33 BitTorrent 协议如何确保新节点能从其他种子随机获得部分数据？   通过扩展协议（Extension Protocol），客户端可以向其它对等方发送元データ文件的块。这些信息被称为“info-dictionary”，足以让用户加入特定的swarm并开始下载所需内容。在这种情况下，新节点不需要先从tracker服务器获取种子，而是直接与其他已有连接建立联系，从而实现了无中心化、去中间人（P2p）的分布式文件共享。\n",
            "core question:核心問題：那個是甚麼影片阿？\n",
            "search keywords:影片\n",
            "\n",
            "\n",
            "34 影片可以指電影、錄音帶或視頻檔案。\n",
            "core question:提取核心問題：\n",
            "\n",
            "究竟在這些眾多乳酪口味當中，令戈芬氏鳳頭鸚鵡展現出顯著偏好的是哪一種呢？\n",
            "search keywords:戈芬氏鳳頭鸚鵡、乳酪口味\n",
            "\n",
            "\n",
            "35 蓝莓口味豆浆乳酪\n",
            "core question:核心問題：最後這隻企鵝寶嬰兒的名字是什麼？\n",
            "search keywords:最後、企鵝寶嬰兒\n",
            "\n",
            "\n",
            "36 最後這隻企鵝寶嬰兒的名字是露比。\n",
            "core question:核心問題：國立臺灣大學物理治療學系的正常修業年限為幾個月？\n",
            "search keywords:物理治療學系 # 國立臺灣大學\n",
            "\n",
            "\n",
            "37 國立臺灣大學物理治療學系的正常修業年限為六個月。\n",
            "core question:核心問題：《BanG Dream!》中哪位角色笑聲習慣是「呼嘿 嘻」？\n",
            "search keywords:《BanG Dream!》；角色笑聲習慣是「呼嘿 嘻」\n",
            "\n",
            "\n",
            "38 《BanG Dream!》中哪位角色笑聲習慣是「呼嘿 嘻」？  答案：麻彌\n",
            "core question:提取核心問題：日本戰國時代被稱為「甲斐之虎」的人物是誰？\n",
            "search keywords:甲斐之虎、誰的\n",
            "\n",
            "\n",
            "39 甲斐之虎是誰？\n",
            "core question:核心問題：王肥貓同學最有可能去修哪一門課？\n",
            "search keywords:王肥貓　同學  最有可能去修課程\n",
            "\n",
            "\n",
            "40 王肥貓同學最有可能去修的課是「PythonGIS防災」或 「科学計算平台」。\n",
            "core question:提取核心問題：2024年的第42回《極限體能王SASUKE》在哪一天首播？\n",
            "search keywords:2024年 · 第42回  · 極限體能王SASUKE   \n",
            "首播日期\n",
            "\n",
            "\n",
            "41 2024年的第42回《極限體能王SASUKE》在7月21日首播。\n",
            "core question:核心問題：出身於利嘉部落，後來成為初鹿德布拉克頭目的漢人名為誰？\n",
            "search keywords:利嘉部落 # 初鹿德布拉克頭目 依據資料\n",
            "\n",
            "\n",
            "42 根據你的問題，我找到了相關資訊。  答案：林朝棟  解釋: 林 朝 棊 是一位著名的漢人頭目，他出身於利嘉部落，後來成為初鹿德布拉克的一個重要人物。\n",
            "core question:核心問題：《BanG Dream! Ave Mujica》的片頭曲是哪一首？\n",
            "search keywords:片頭曲、BanG Dream! Ave Mujica\n",
            "\n",
            "\n",
            "43 《BanG Dream! Ave Mujica》的片頭曲是「KiLL KiSS」。\n",
            "core question:核心問題：Linux作業系統最早於哪一年首次發布？\n",
            "search keywords:最早；Linux作業系統;首次發布年份\n",
            "\n",
            "\n",
            "44 Linux作業系統最早於1991年首次發布。\n",
            "core question:核心問題：Likavung 的中文名稱為何？\n",
            "search keywords:Likavung\n",
            "\n",
            "\n",
            "45 呂家望Likavung的中文名稱為「利嘉部落」。\n",
            "core question:核心問題：紅茶是全發酵還是不完全或不經過完整的黑醋酸菌（Aspergillus niger）等微生物分解後再進行部分氧化而成？\n",
            "search keywords:紅茶、全發酵 # 不完全或不經過完整的黑醋酸菌（Aspergillus niger）等微生物分解後再進行部分氧化而成\n",
            "\n",
            "\n",
            "46 紅茶是全發酵的。\n",
            "core question:核心問題：以「真紅眼黑龍」與 「 黑魔導 」作為融合素材的 融 合怪獸 是什麼？\n",
            "search keywords:真紅眼黑龍、 黑魔導\n",
            "\n",
            "\n",
            "47 真紅眼黑龍與「 黑魔導 」作為融合素材的 融 合怪獸 是 真 鐵騎士－基亞‧弗裡德。\n",
            "core question:豐田萌繪在《BanG Dream!》企劃中，擔任哪個角色的聲優？\n",
            "search keywords:豐田萌繪；BanG Dream！;角色\n",
            "\n",
            "\n",
            "48 豐田萌繪在《BanG Dream!》企劃中，擔任松原花音的聲優。\n",
            "core question:核心問題：Rugby Union 中，9 號球員的正式名稱為何？\n",
            "search keywords:9 號  Rugby Union\n",
            "\n",
            "\n",
            "49 9 號球員的正式名稱為傳接鋒（Scrum-half）。\n",
            "core question:核心問題：曾被視為太陽系中的行星，最終降格成矮 行 星的哪個球？\n",
            "search keywords:太陽系、行星、中矮 行 星\n",
            "\n",
            "\n",
            "50 曾被視為太陽系中的行星，最終降格成矮 行 星的球是冥王土\n",
            "core question:核心問題：臺灣最早成立的野生動物救傷單位位於哪個行政區內？\n",
            "search keywords:最早  野生動物救傷單位   臺灣\n",
            "\n",
            "\n",
            "51 根據提供的資訊，臺灣最早成立野生動物救傷單位位於南投縣集集中。\n",
            "core question:核心問題：特生中心在2023年改名後的名字是什麼？\n",
            "search keywords:特生中心改名後名字 # 2023年\n",
            "\n",
            "\n",
            "52 農業部生物多樣性研究所在2023年改名後的名字是「臺灣生態研究院」\n",
            "core question:模型名稱\n",
            "search keywords:模型名稱\n",
            "\n",
            "\n",
            "53 大模型命名的秘密：名称与后缀背后的意义  1. 名称篇 国内的大 모델名字多融入中国传统文化，既有古风雅韵，也体现了科技创新。 旧词新用派文心一言（百度）通义千问 （阿里） 这些名直接取材于中历史和神话人物，有厚重感也易引发共鸣。  2. 后缀篇 模型后 缩表示技术规格与应用方向，包括： 版本号：如Llama 3 和 ChatGPT4。 参数量级：“B”代表十亿，“T ”指万億。例如70 B 表示有七 十 億 个参 数 ，1 T 指一 万 個 參 數 。 数据集和语言，如“Chinese”。  总结，无论是名称的独特性还是后缀技术细节，大模型命名都反映了背后的文化理念与技 术 创新。\n",
            "core question:提取核心問題：太陽系中體積最大的行星是哪一顆？\n",
            "search keywords:太陽系 · 最大行星\n",
            "\n",
            "\n",
            "54 太陽系中體積最大的行星是木衛。\n",
            "core question:核心問題：哪一族的語言與其他原住民族群最遙遠？\n",
            "search keywords:最遙遠；原住民族群; 語言\n",
            "\n",
            "\n",
            "55 根據資料，與其他原住民族群最遙遠的語言是達悟族使用之「鄒」、「卑南」，以及泰雅、賽德克等部落所屬於北台灣和中臺灣地區。\n",
            "core question:提取核心問題：講出這句話的老師是誰？\n",
            "search keywords:老師；是誰\n",
            "\n",
            "\n",
            "56 孔子的老师是谁？\n",
            "core question:核心問題：「embiyax namu kana」是哪一臺灣原住民族的打招呼用語？\n",
            "search keywords:embiyax namu kana；臺灣原住民族 ；打招呼用語\n",
            "\n",
            "\n",
            "57 答案：Embiyax namu kana 是太魯閣族的打招呼用語。  解釋: 根據網站上的資料，「E mb iy ax n am u ka na」是 太 魯 閩 類 的 打 招 呼 用 語。\n",
            "core question:提取核心問題：「鄒與布農，永久美麗」這句話哪個部落息 息相關？\n",
            "search keywords:鄒；布農; 部落\n",
            "\n",
            "\n",
            "58 根據提供的資訊，「鄒與布農, 永久美麗」這句話是關於哪個部落息 息相關？  答案：雖然文獻中提到許多有趣的事實，但並沒有明確指出該詞彙所對應的是什麼一種族群或社區。然而，根據提供的資訊，我們可以推測這句話可能與布農部落息 息相關，因為文獻中提到許多關於 布农 部 落 的 資 断 介 紹 和 文 化 特 色 等 內 宜 。\n",
            "core question:核心問題：女主角隱藏的冒險者身份是甚麼？\n",
            "search keywords:女主角、冒險者身份\n",
            "\n",
            "\n",
            "59 雷特·费纳（Rent Faina）是一名成为不死者的冒险者，他的身份是骷髅。\n",
            "core question:核心問題：姊妹 Tuku 創建了哪一個部落？\n",
            "search keywords:姊妹 Tuku、部落\n",
            "\n",
            "\n",
            "60 姊妹 Tuku 創建了利嘉部落。\n",
            "core question:核心問題：2005 年播出的電視劇《終極一班》中的「KO榜」第一名是誰？\n",
            "search keywords:終極一班、KO榜、一名\n",
            "\n",
            "\n",
            "61 KO榜第一名是汪大东。\n",
            "core question:核心問題：Linux kernel 的 CFS process scheduler 使用何種資料結構儲存排程相關資訊？\n",
            "search keywords:Linux kernel CFS process scheduler 使用紅黑樹（Red-Black Tree）\n",
            "\n",
            "\n",
            "62 Linux核心的紅黑樹  **什麼是Red-Black Tree?**  红-black树是一種自平衡二叉查找數據結構，能夠保持相對穩定的搜索、插入和刪除操作時間複雜度。它在 Linux 核心中被用於實現完全公正調製器（CFS）的紅黑樹資料庫。  **Red-Black Tree 的特性**  1.  每個節點都有兩種顏色：红或黒。 2\\. 所有的葉子都是空的，且是 BLACK 节点。  3\\). 根节点为BLACK;每个叶结节（END）和根之外所有非终端NULL指针均设定为空黑色的；若一个内部NULl键值对存在，则其颜色必须与父树相同。 4\\. 如果任一子数的高度大于0，且它是红节点，那么这个子的兄弟一定也是个RED结点（即同级别不能有两个连续着为red 的node）。 5\\). 对每一个给定根r所表示二叉查找树T中的所有叶值都存在以下性质：如果k代表黑色，n则是红色的，则从任一节点到其子孙的最长简单路径上至多有两个结点被标记为“RED”。  **紅black樹在Linux核心中**  Red-Black Tree 在 Linux 核心中的應用主要是在完全公正調製器（CFS） 中。 C FS 是一個使用 Red-black tree 的自平衡二叉查找數據結構，來實現進程的排隊和選擇。  在Linux核心中，每個CPU都有一棵紅黑樹，用於儲存當前正在等待執行或已經被放入就緒列表中的所有任務。每次調度器需要從這些列出一個新的工作時，它會根據虛拟运行时间（vruntime）來選擇最小的節點。  **紅黑樹在Linux核心中優勢**  1\\. **自平衡**: Red-Black Tree 能夠保持相對穩定的搜索、插入和刪除操作時間複雜度，從而提高 Linux 核心中的調製器效率。 2\\). \n",
            "core question:核心問題：諾曼第登陸的作戰代號是什麼？\n",
            "search keywords:諾曼第登陸;作戰代號\n",
            "\n",
            "\n",
            "63 諾曼第登陸的作戰代號是「霸王」行動（Operation Overlord），又稱為D日。\n",
            "core question:提取核心問題：《Cytus II》遊戲中「Body Talk」是哪位角色的歌曲？\n",
            "search keywords:《Cytus II》  Body Talk   角色\n",
            "\n",
            "\n",
            "64 《Cytus II》遊戲中「Body Talk」是Noursi的歌曲。\n",
            "core question:核心問題：李琳山教授的演講又被稱為什麼？\n",
            "search keywords:李琳山教授；演講\n",
            "\n",
            "\n",
            "65 李琳山教授的演講又被稱為「反智運動論文」\n",
            "core question:核心問題：RTX 5090 顯卡的 VRAM 是多少？\n",
            "search keywords:顯卡、RTX 5090 、VRAM\n",
            "\n",
            "\n",
            "66 GeForce RTX 5090 顯卡的 VRAM 是32GB。\n",
            "core question:提取核心問題：2024年世界棒球12強賽冠軍為哪一隊？\n",
            "search keywords:世界棒球 12強賽 冠軍\n",
            "\n",
            "\n",
            "67 2024年世界棒球12強賽冠軍為中華臺北。\n",
            "core question:核心問題：中國四大奇書是哪幾本？\n",
            "search keywords:中國 # 四大奇書\n",
            "\n",
            "\n",
            "68 中国四大奇书是《水滸傳》、《三國演義》， 《金瓶梅》（又稱為「西遊記」）\n",
            "core question:提取核心問題：子時在24小时制中的時間範圍是什麼？\n",
            "search keywords:子時；24小時間段\n",
            "\n",
            "\n",
            "69 子时出生如何确定時辰？   一天的24小时，两个时间段是一个时候。子的時間是从晚上23点到凌晨1點。  早上的子は前半夜11至13分为“子の”；午后的也是12-14分钟為「丑の」;下面的子时則在21時30～22:00之間，属于第二天的开始。\n",
            "core question:核心問題：在作業系統中，避免要錯過時限來完成任務的排程演算法稱為什麼？\n",
            "search keywords:最關鍵字：排程演算法、時限\n",
            "\n",
            "\n",
            "70 在作業系統中，避免要錯過時限來完成任務的排程演算法稱為「就近先行」或是 「最短工作優 先」（SJF）原則。\n",
            "core question:該代號「C8763」在原作中對應於黑乃亞·葛拉西婭持有的劍技。\n",
            "search keywords:黑乃亞·葛拉西婭；劍技\n",
            "\n",
            "\n",
            "71 根据检索结果，C8763代号对应于黑乃亞·葛拉西婭持有的劍技。\n",
            "core question:核心問題：劇中之地名「柴城」位於現今的哪個行政區劃？\n",
            "search keywords:柴城、地名、\n",
            "依據、\n",
            "\n",
            "行政區劃\n",
            "\n",
            "\n",
            "72 車城鄉位於屏東縣內西南方，北臨枋山、狮子和牡丹三個行政區。\n",
            "core question:核心問題：Google Colab的訂閱制中，若要使用A100高級GPU需要哪個計畫？\n",
            "search keywords:Google Colab  A100 高級 GPU 計畫\n",
            "\n",
            "\n",
            "73 Google Colab 的訂閱制中，若要使用 A100 高級 GPU 需求的是 Pro+ 計畫。\n",
            "core question:核心問題：李宏毅老師開設的機器學習課程屬於哪個學院？\n",
            "search keywords:李宏毅  老師   機器學習課程    學院\n",
            "\n",
            "\n",
            "74 李宏毅老師開設的機器學習課程屬於電資學院。\n",
            "core question:核心問題：雪江同學選擇第一個策略，至少要修多少分才不用簽減免申請書？\n",
            "search keywords:雪江　第一個策略  最少分數   減免申請書\n",
            "\n",
            "\n",
            "75 根據你的問題，我找到了相關資訊。  雪江同學選擇第一個策略，至少要修多少分才不用簽減免申請書？  答案：一般來說，大多數學校的政策是，如果考試成績達到一定標準（通常在80-90之間），就可以避開降低獎勵金額或取消學費補助。具體而言，這個分界線可能會根據不同院校、課程和個人條件有所變動。  然而，沒有明確的答案能夠涉及所有學校的情況，因為這取決於各自不同的政策規定。如果你想知道特定的數值，我建議查詢相關學府或教育部門。\n",
            "core question:核心問題：Neuro-sama 的最初 Live2D 模型是使用 VTube Studio 哪個角色？\n",
            "search keywords:Neuro-sama · VTube Studio\n",
            "\n",
            "\n",
            "76 Neuro-sama 的最初 Live2D 模型是使用 VTube Studio Momose Hiyori。\n",
            "core question:核心問題：從零開始的異世界生活 第三季動畫中，劫持愛蜜莉雅並想取其為妻的人是誰？\n",
            "search keywords:愛蜜莉雅；劫持者; 第三季\n",
            "\n",
            "\n",
            "77 答案：強欲司教  第三季的故事主要講述在聯手擊敗了大兔後，主角們暫且度過了一年的安穩時光。然而，這樣的情況卻被使節送來的一封信所打破，並導致艾蜜莉亞（愛美麗雅）應邀前往水門都市朴利斯提拉。在這裡，她遇到了強欲司教，他想將她作為自己的妻子，展開了一系列的故事。\n",
            "core question:核心問題：海綿寶宝在第五季《失蹤記》中，在哪個城市擊敗刺破泡沫紅眼幇？\n",
            "search keywords:海綿寶宝 · 第五季 《失蹤記》 依據... 在哪個城市 駁破刺眼幇\n",
            "\n",
            "\n",
            "78 根據你的問題，我找到了相關資訊。  海綿寶宝在第五季《失蹤記》中，在哪個城市擊敗刺破泡沫紅眼幇？  答案是：布魯克林市  理由是在整部動畫系列裡，該集的背景設定為美國紐約州長島地區，而海綿寶宝在這集中與敵人進行鬥爭，並最終取得勝利。\n",
            "core question:核心問題：玉米是單子葉還是不雙子的植物？\n",
            "search keywords:玉米、單子葉植物\n",
            "\n",
            "\n",
            "79 玉米是單子葉植物。\n",
            "core question:核心問題：中華民國陸軍的军歌前六字是什麼？\n",
            "search keywords:中華民國陸軍 # 軍歌\n",
            "\n",
            "\n",
            "80 中華民國陸軍的军歌前六字是「風雲起，山河動」。\n",
            "core question:核心問題：台大電資學院哪個系規定物理、化學以及生物科目可以只擇一修習即可？\n",
            "search keywords:物理、化學，生物科目，一修習\n",
            "\n",
            "\n",
            "81 根據提供的資訊，台大電機學院規定物理、化學以及生物科目可以只擇一修習即可。\n",
            "core question:核心問題：五個地形中哪些位於月球背面？\n",
            "search keywords:月球背面；地形\n",
            "\n",
            "\n",
            "82 月球背面的地形包括：  *   陨石坑：比正面多得很多     *  月海（maria）：只有1%的表面积被覆盖，相对于31.2%         + 正如从勘探者获得的地化学建模所证明的一样，与产热元素浓度较高有关。 *   火山口：比正面少     *  月球熔岩流可能掩埋了陨石坑，导致背面的可见性降低。\n",
            "core question:核心問題：《C♯小調第14號鋼琴奏鳴曲》較為人知的別稱是什麼？\n",
            "search keywords:C♯小調第14號鋼琴奏鳴曲；較為人知的別稱\n",
            "\n",
            "\n",
            "83 《C♯小調第14號鋼琴奏鳴曲》較為人知的別稱是什麼？  答案：月光  理由根據檢索結果，該作品被命名爲「幻想式的小提督」，但評論家路德維希·萊爾斯塔勃在1832年將其改成《C♯小調第14號鋼琴奏鳴曲》的別稱為月光。\n",
            "core question:提取核心問題：阿米斯音樂節是由哪位歌手所舉辦？\n",
            "search keywords:阿米斯音樂節 · 歌手\n",
            "\n",
            "\n",
            "84 舒米恩是阿美族歌手，他創辦了以原住民族文化為主體的「 阿 米斯音樂節 」。\n",
            "core question:核心問題：Poppy Playtime - Chapter 4 中的黏土人名字是甚麼？\n",
            "search keywords:黏土人名字、Poppy Playtime\n",
            "\n",
            "\n",
            "85 Poppy Playtime - Chapter 4 中的黏土人名字是 Huggy Wuffy。\n",
            "core question:核心問題：賓茂村屬於何一行政區劃？\n",
            "search keywords:賓茂村；行政區劃\n",
            "\n",
            "\n",
            "86 賓茂村屬於金峰鄉的行政區劃。\n",
            "core question:核心問題：米開朗基羅創作的《大衛》雕像最初是在何處被發現或展出？\n",
            "search keywords:米開朗基羅 · 大衛雕像 # 最初在哪裡\n",
            "\n",
            "\n",
            "87 米开朗基罗的大卫雕像最初是在佛洛伦萨的市政厅旧宫入口被发现或展出。\n",
            "core question:提取核心問題：除了蔣中正之外，曾短暫晉升特級上將的另一位軍事領袖是誰？\n",
            "search keywords:蔣中正 # 短暫晉升特級上將的軍事領袖是誰\n",
            "\n",
            "\n",
            "88 李烈钧\n",
            "core question:核心問題：線上遊戲「英雄聯盟」2012年第二賽季世界大赛的總冠軍是哪一個戰隊？\n",
            "search keywords:英雄聯盟 2012 年 第二 賽季 世界 大賽 績效 最佳戰隊\n",
            "\n",
            "\n",
            "89 2012年英雄聯盟世界大賽的總冠軍是台北暗殺者（Taipei Assassins）。\n",
            "core question:提取核心問題：在日本麻將中，非莊家一開始的手牌有幾張？\n",
            "search keywords:日本麻將  非莊家 手牌 數量\n",
            "\n",
            "\n",
            "90 在日本麻將中，非莊家一開始的手牌有13張。\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Fill in your student ID first.\n",
        "STUDENT_ID = \"b12901166\"\n",
        "\n",
        "STUDENT_ID = STUDENT_ID.lower()\n",
        "with open('./public.txt', 'r') as input_f:\n",
        "    questions = input_f.readlines()\n",
        "    questions = [l.strip().split(',')[0] for l in questions]\n",
        "    for id, question in enumerate(questions, 1):\n",
        "        # if Path(f\"./{STUDENT_ID}_{id}.txt\").exists():\n",
        "            # continue\n",
        "        answer = await pipeline(question)\n",
        "        answer = answer.replace('\\n',' ')\n",
        "        print(id, answer)\n",
        "        with open(f'./{STUDENT_ID}_{id}.txt', 'w') as output_f:\n",
        "            print(answer, file=output_f)\n",
        "\n",
        "with open('./private.txt', 'r') as input_f:\n",
        "    questions = input_f.readlines()\n",
        "    for id, question in enumerate(questions, 31):\n",
        "        # if Path(f\"./{STUDENT_ID}_{id}.txt\").exists():\n",
        "            # continue\n",
        "        answer = await pipeline(question)\n",
        "        answer = answer.replace('\\n',' ')\n",
        "        print(id, answer)\n",
        "        with open(f'./{STUDENT_ID}_{id}.txt', 'a') as output_f:\n",
        "            print(answer, file=output_f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GmLO9PlmEBPn"
      },
      "outputs": [],
      "source": [
        "# Combine the results into one file.\n",
        "with open(f'./{STUDENT_ID}.txt', 'w') as output_f:\n",
        "    for id in range(1,91):\n",
        "        with open(f'./{STUDENT_ID}_{id}.txt', 'r') as input_f:\n",
        "            answer = input_f.readline().strip()\n",
        "            print(answer, file=output_f)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
